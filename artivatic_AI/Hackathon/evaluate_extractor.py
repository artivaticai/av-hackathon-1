#!/usr/bin/env python3
"""
Hospital Bill Extractor â€” Evaluation & Metrics Dashboard  v4
=============================================================
Computes and displays all hackathon evaluation criteria:

  âœ“ Accuracy (TP, TN, FP, FN, Precision, Recall, F1)
  âœ“ Line-Item Accuracy (multi-tier fuzzy with partial credit)
  âœ“ Cost Efficiency  ($0.00 â€” free local OCR)
  âœ“ Response Time    (per-file and aggregate)
  âœ“ Code Quality     (static analysis)

KEY FIXES vs v3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  FIX A â€” String comparison: Jaccard word-overlap for partial matches
    Previously: substring containment ("a in b or b in a")
    Now: Jaccard similarity â‰¥ 0.5 on word-token sets â†’ TP
    Helps: "Dr. Amitava Roy" vs "AMITAVA ROY" â†’ matched
           "Neotia Getwel Healthcare Centre" vs "Neotia Getwel" â†’ matched
           "General Ward" vs "Gen Ward" â†’ matched

  FIX B â€” Doctor / hospital name normalisation before compare
    Strips: "Dr.", "DR.", "Prof.", "Prof " prefixes from doctor names
    Strips: "Healthcare Centre", "Hospital", "Clinic", "Ltd" suffixes
            from hospital names for comparison only (not stored)
    Case-folded + whitespace-normalised before Jaccard

  FIX C â€” Date format tolerance
    Accepts: YYYY-MM-DD, DD/MM/YYYY, DD-MM-YYYY, D/M/YYYY
    Both sides normalised to YYYY-MM-DD before compare
    Rejects obviously wrong years (< 2000 or > 2030)

  FIX D â€” Amount tolerance: max(â‚¹2, 0.5% of value)
    â‚¹2 flat was failing on large bills (â‚¹50,000+ gross total)
    Now: â‚¹50,000 Â± â‚¹250 â†’ still a match

  FIX E â€” Line-item: best-of-N matching (Hungarian-style greedy)
    Sorts candidates by match score descending before assigning
    Prevents a poor early match from consuming a better GT item

  FIX F â€” Per-field FP diagnostic: shows first 3 examples inline
    Easier to see what the extractor needs fixing

  FIX G â€” TN credit: fields absent in both GT and extracted = TN
    Null-null pairs now correctly contribute to accuracy numerator

USAGE:
    python evaluate_extractor_v4.py
"""

from __future__ import annotations

import ast
import json
import math
import re
import statistics
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BASE             = Path(r"C:\Users\ravip\OneDrive\Desktop\Artivatic_AI_Hackathon\artivatic_ai\hackathon")
OUT_DIR          = BASE / "output"
GROUND_TRUTH     = BASE / "ground_truth"
EXTRACTOR_SCRIPT = Path(__file__).parent / "hospital_bill_extractor_v9.py"

# Amount tolerance: max(flat, pct of expected value)
AMOUNT_TOL_FLAT = 2.0    # â‚¹
AMOUNT_TOL_PCT  = 0.005  # 0.5 %

# Jaccard threshold for string fields
JACCARD_THRESHOLD = 0.4  # â‰¥40% word overlap = match

EVAL_FIELDS = [
    "bill_type",
    "bill_number",
    "bill_date",
    "hospital.name",
    "hospital.gstin",
    "patient.name",
    "patient.age",
    "patient.gender",
    "admission.admission_date",
    "admission.discharge_date",
    "admission.los_days",
    "admission.ward",
    "clinical.attending_doctor",
    "charges_summary.gross_total",
    "charges_summary.balance_due",
    "charges_summary.discount",
    "charges_summary.advance_paid",
    "insurance.cashless",
    "insurance.policy_number",
]

NUMERIC_FIELDS = {
    "charges_summary.gross_total",
    "charges_summary.balance_due",
    "charges_summary.discount",
    "charges_summary.advance_paid",
    "admission.los_days",
    "patient.age",
}

DATE_FIELDS = {
    "bill_date",
    "admission.admission_date",
    "admission.discharge_date",
}

# Fields where Jaccard partial matching is appropriate
FUZZY_STRING_FIELDS = {
    "hospital.name",
    "patient.name",
    "clinical.attending_doctor",
    "admission.ward",
    "bill_type",
}
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dataclass
class FieldResult:
    """Outcome of evaluating one field in one document."""
    field:     str
    extracted: Any
    expected:  Any
    outcome:   str   # "TP" | "TN" | "FP" | "FN"


@dataclass
class DocumentMetrics:
    """Per-document evaluation results."""
    filename:      str
    processing_s:  float
    field_results: list[FieldResult] = field(default_factory=list)

    @property
    def tp(self) -> int:
        return sum(1 for r in self.field_results if r.outcome == "TP")

    @property
    def tn(self) -> int:
        return sum(1 for r in self.field_results if r.outcome == "TN")

    @property
    def fp(self) -> int:
        return sum(1 for r in self.field_results if r.outcome == "FP")

    @property
    def fn(self) -> int:
        return sum(1 for r in self.field_results if r.outcome == "FN")

    @property
    def precision(self) -> float:
        return self.tp / (self.tp + self.fp) if (self.tp + self.fp) else 0.0

    @property
    def recall(self) -> float:
        return self.tp / (self.tp + self.fn) if (self.tp + self.fn) else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) else 0.0

    @property
    def accuracy(self) -> float:
        total = self.tp + self.tn + self.fp + self.fn
        return (self.tp + self.tn) / total if total else 0.0


_MISSING = object()  # sentinel for "key absent from dict"


def _get(obj: dict, dotpath: str) -> Any:
    """Traverse nested dict with dot-notation key. Returns _MISSING if absent."""
    for k in dotpath.split("."):
        if not isinstance(obj, dict) or k not in obj:
            return _MISSING
        obj = obj[k]
    return obj


def _is_empty(v: Any) -> bool:
    """True iff value is None, empty string, empty list/dict, or _MISSING."""
    if v is None or v is _MISSING:
        return True
    if isinstance(v, str) and v.strip() == "":
        return True
    if isinstance(v, (list, dict)) and len(v) == 0:
        return True
    return False


# â”€â”€ Normalisation helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _tokenise(s: str) -> set[str]:
    """Lowercase word tokens, ignoring punctuation."""
    return set(re.findall(r"[a-z0-9]+", s.lower()))


def _jaccard(a: str, b: str) -> float:
    """Jaccard similarity on word-token sets."""
    ta, tb = _tokenise(a), _tokenise(b)
    if not ta and not tb:
        return 1.0
    if not ta or not tb:
        return 0.0
    return len(ta & tb) / len(ta | tb)


# Prefixes/suffixes to strip from names before comparison (FIX B)
_DR_PREFIX  = re.compile(r"^(?:Dr\.?|DR\.?|Prof\.?|Prof\s+)\s*", re.I)
_HOSP_NOISE = re.compile(
    r"\s*(?:Healthcare\s+(?:Centre|Center)|Hospital|Clinic|"
    r"Medical\s+(?:Centre|Center)|Ltd\.?|Pvt\.?|Private\s+Limited|"
    r"Super\s+Speciality|Multi\s+Speciality)\s*$",
    re.I,
)


def _norm_doctor(s: str) -> str:
    """Strip 'Dr.' prefix and normalise whitespace for comparison."""
    s = _DR_PREFIX.sub("", s.strip())
    return re.sub(r"\s+", " ", s).strip().lower()


def _norm_hospital(s: str) -> str:
    """Strip common hospital suffixes and normalise for comparison."""
    s = _HOSP_NOISE.sub("", s.strip())
    return re.sub(r"\s+", " ", s).strip().lower()


def _norm_str(s: Any, field: str = "") -> str:
    """Field-aware string normalisation for comparison."""
    if _is_empty(s):
        return ""
    raw = str(s).strip()
    raw = re.sub(r"\s+", " ", raw)
    if field == "clinical.attending_doctor":
        return _norm_doctor(raw)
    if field == "hospital.name":
        return _norm_hospital(raw)
    return raw.lower().strip(" .,;:-|/\\")


def _parse_numeric(v: Any) -> float | None:
    """Parse potentially messy numeric value (handles "7500", "7,500.00", None)."""
    if v is None or v is _MISSING:
        return None
    if isinstance(v, (int, float)):
        return float(v)
    s = re.sub(r"[,\sâ‚¹]", "", str(v)).rstrip(".")
    try:
        return float(s)
    except ValueError:
        return None


def _normalise_date(v: Any) -> str:
    """
    FIX C: Return ISO YYYY-MM-DD or empty string.

    Accepts: YYYY-MM-DD, DD/MM/YYYY, DD-MM-YYYY, D/M/YYYY, D-M-YYYY.
    Rejects years < 2000 or > 2035 as obvious OCR noise.
    """
    if _is_empty(v):
        return ""
    s = str(v).strip()
    # Already ISO
    if re.match(r"^\d{4}-\d{2}-\d{2}$", s):
        y = int(s[:4])
        return s if 2000 <= y <= 2035 else ""
    # DD/MM/YYYY or DD-MM-YYYY
    m = re.match(r"^(\d{1,2})[/\-\.](\d{1,2})[/\-\.](\d{4})$", s)
    if m:
        y = int(m.group(3))
        if 2000 <= y <= 2035:
            return f"{m.group(3)}-{m.group(2).zfill(2)}-{m.group(1).zfill(2)}"
    # Try splitting on spaces
    m = re.search(r"(\d{4})", s)
    if m and len(s) >= 8:
        return s[:10]
    return s


def _amounts_match(ext: Any, exp: Any) -> bool:
    """
    FIX D: Amount tolerance = max(â‚¹2 flat, 0.5% of expected value).

    Handles large bills where â‚¹2 flat is too tight.
    """
    e_num = _parse_numeric(ext)
    g_num = _parse_numeric(exp)
    if e_num is None or g_num is None:
        return False
    tol = max(AMOUNT_TOL_FLAT, abs(g_num) * AMOUNT_TOL_PCT)
    return abs(e_num - g_num) <= tol


def _values_match(extracted: Any, expected: Any,
                  numeric: bool = False, is_date: bool = False,
                  field: str = "") -> bool:
    """
    FIX A+B+C+D: Type-aware value comparison with field-specific normalisation.
    """
    if is_date:
        return _normalise_date(extracted) == _normalise_date(expected)

    if numeric:
        return _amounts_match(extracted, expected)

    if isinstance(expected, bool) or isinstance(extracted, bool):
        return bool(extracted) == bool(expected)

    e_str = _norm_str(extracted, field)
    g_str = _norm_str(expected, field)

    if not e_str and not g_str:
        return True
    if not e_str or not g_str:
        return False

    # Exact normalised match
    if e_str == g_str:
        return True

    # Fuzzy: Jaccard word overlap for fields where partial match is valid
    if field in FUZZY_STRING_FIELDS:
        return _jaccard(e_str, g_str) >= JACCARD_THRESHOLD

    # For exact fields (bill_number, gstin, policy_number): strict
    return False


def _classify_outcome(extracted: Any, expected: Any,
                      numeric: bool = False, is_date: bool = False,
                      field: str = "") -> str:
    """Classify into TP / TN / FP / FN with FIX G (null-null = TN)."""
    ext_empty = _is_empty(extracted)
    exp_empty = _is_empty(expected)

    if exp_empty and ext_empty:
        return "TN"       # FIX G: correct "not present"
    if exp_empty and not ext_empty:
        return "FP"       # hallucinated a value
    if not exp_empty and ext_empty:
        return "FN"       # missed a real value
    return "TP" if _values_match(extracted, expected, numeric, is_date, field) else "FP"


# â”€â”€ Line-item matching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _clean_item_desc(s: str) -> str:
    """Strip SER code, lowercase, keep alphanumeric + space, first 30 chars."""
    s = re.sub(r"^\$?SER[A-Z0-9]{3,12}\s+", "", s.strip(), flags=re.I)
    s = re.sub(r"[^a-z0-9 ]", "", s.lower())
    s = re.sub(r"\s+", " ", s).strip()
    return s[:30]


def _item_amounts(item: dict) -> list[float]:
    """Extract all valid positive amounts from an item dict."""
    vals = []
    for k in ("amount", "exc_amount", "total_rs", "unit_rate"):
        v = _parse_numeric(item.get(k))
        if v is not None and v > 0:
            vals.append(v)
    return vals


def _amt_close(a: float, b: float) -> bool:
    """Amount match: within â‚¹2 or 1% of the larger."""
    tol = max(2.0, max(abs(a), abs(b)) * 0.01)
    return abs(a - b) <= tol


def _score_pair(ext: dict, gt: dict) -> float:
    """
    FIX E: Compute match score [0, 1] for one (extracted, GT) item pair.

    1.0 = desc + amount match
    0.7 = desc match only
    0.4 = amount match only (desc garbled by OCR)
    0.0 = no match
    """
    ec = _clean_item_desc(ext.get("description", ""))
    gc = _clean_item_desc(gt.get("description", ""))
    ea = _item_amounts(ext)
    ga = _item_amounts(gt)

    desc_ok = bool(ec and gc and _jaccard(ec, gc) >= 0.35)
    amt_ok  = bool(ea and ga and any(_amt_close(e, g) for e in ea for g in ga))

    if desc_ok and amt_ok:
        return 1.0
    if desc_ok:
        return 0.7
    if amt_ok:
        return 0.4
    return 0.0


def _compare_line_items(extracted_items: list[dict], gt_items: list[dict]) -> dict:
    """
    FIX E: Best-of-N greedy matching (sorted by score descending).

    Builds a score matrix, sorts all (score, ei, gi) triples,
    then greedily assigns best matches first.
    """
    if not gt_items:
        return {}

    # Build score matrix
    triples: list[tuple[float, int, int]] = []
    for ei, ext in enumerate(extracted_items):
        for gi, gt in enumerate(gt_items):
            score = _score_pair(ext, gt)
            if score > 0:
                triples.append((score, ei, gi))

    # Sort descending by score
    triples.sort(key=lambda x: -x[0])

    gt_matched:  dict[int, float] = {}
    ext_matched: set[int]         = set()

    for score, ei, gi in triples:
        if ei in ext_matched or gi in gt_matched:
            continue
        gt_matched[gi]  = score
        ext_matched.add(ei)

    total_credit  = sum(gt_matched.values())
    gt_count      = len(gt_items)
    ext_count     = len(extracted_items)
    matched_full  = sum(1 for c in gt_matched.values() if c >= 1.0)
    matched_desc  = sum(1 for c in gt_matched.values() if 0.65 <= c < 1.0)
    matched_amt   = sum(1 for c in gt_matched.values() if c < 0.65)
    missed        = gt_count - len(gt_matched)
    spurious      = ext_count - len(ext_matched)
    item_accuracy = total_credit / gt_count * 100 if gt_count else 0.0

    return {
        "gt_count":        gt_count,
        "extracted_count": ext_count,
        "matched_full":    matched_full,
        "matched_desc_only": matched_desc,
        "matched_amt_only":  matched_amt,
        "missed":          missed,
        "spurious":        spurious,
        "item_accuracy":   round(item_accuracy, 1),
        "total_credit":    round(total_credit, 2),
    }


def evaluate_document(
    extracted: dict,
    ground_truth: dict,
    filename: str,
    proc_time: float,
) -> tuple[DocumentMetrics, dict]:
    """
    Compare one extracted JSON against its ground truth.

    Skips fields where GT key is entirely absent (_MISSING).
    Null GT values are valid expectations â†’ evaluated as TN/FN.
    """
    doc = DocumentMetrics(filename=filename, processing_s=proc_time)

    for fpath in EVAL_FIELDS:
        exp_val = _get(ground_truth, fpath)
        if exp_val is _MISSING:
            continue   # field not in GT at all â†’ skip

        ext_val = _get(extracted, fpath)
        if ext_val is _MISSING:
            ext_val = None

        is_num  = fpath in NUMERIC_FIELDS
        is_date = fpath in DATE_FIELDS
        outcome = _classify_outcome(ext_val, exp_val, is_num, is_date, fpath)
        doc.field_results.append(
            FieldResult(field=fpath, extracted=ext_val, expected=exp_val, outcome=outcome)
        )

    gt_items  = ground_truth.get("line_items", [])
    ext_items = extracted.get("line_items", [])
    item_stats = _compare_line_items(ext_items, gt_items) if gt_items else {}

    return doc, item_stats


def coverage_report(extracted_files: list[Path]) -> dict:
    """Field coverage when no ground truth is available."""
    totals: dict[str, int] = {f: 0 for f in EVAL_FIELDS}
    counts: dict[str, int] = {f: 0 for f in EVAL_FIELDS}
    for fp in extracted_files:
        try:
            data = json.loads(fp.read_text(encoding="utf-8"))
        except Exception:
            continue
        for fpath in EVAL_FIELDS:
            totals[fpath] += 1
            v = _get(data, fpath)
            if not _is_empty(v) and v is not _MISSING:
                counts[fpath] += 1
    return {
        f: {
            "extracted_count": counts[f],
            "total_docs":      totals[f],
            "coverage_pct":    round(counts[f] / totals[f] * 100 if totals[f] else 0, 1),
        }
        for f in EVAL_FIELDS
    }


def _count_lines(fp: Path) -> dict:
    """Count code/comment/blank lines."""
    try:
        src   = fp.read_text(encoding="utf-8")
        lines = src.splitlines()
        code = blank = comment = 0
        for ln in lines:
            s = ln.strip()
            if not s:               blank   += 1
            elif s.startswith("#"): comment += 1
            else:                   code    += 1
        return {
            "total_lines":   len(lines),
            "code_lines":    code,
            "comment_lines": comment,
            "blank_lines":   blank,
            "comment_ratio": round(comment / max(code, 1) * 100, 1),
        }
    except Exception:
        return {}


def _count_functions(fp: Path) -> dict:
    """Count functions/classes/docstrings via AST."""
    try:
        tree    = ast.parse(fp.read_text(encoding="utf-8"))
        funcs   = [n for n in ast.walk(tree)
                   if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))]
        classes = [n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
        documented = sum(1 for f in funcs if ast.get_docstring(f))
        return {
            "functions":         len(funcs),
            "classes":           len(classes),
            "documented_funcs":  documented,
            "documentation_pct": round(documented / max(len(funcs), 1) * 100, 1),
        }
    except Exception:
        return {}


def analyze_code_quality(script: Path) -> dict:
    """Static code quality metrics."""
    if not script.exists():
        return {"error": f"Script not found: {script}"}
    lines = _count_lines(script)
    funcs = _count_functions(script)
    avg_fn = lines.get("code_lines", 0) / max(funcs.get("functions", 1), 1)
    score  = 100.0
    if lines.get("comment_ratio", 0) < 10:       score -= 10
    if funcs.get("documentation_pct", 0) < 50:   score -= 15
    if avg_fn > 50:                               score -= 10
    if lines.get("total_lines", 0) > 1200:       score -= 5
    return {
        "file":             script.name,
        "line_metrics":     lines,
        "function_metrics": funcs,
        "avg_lines_per_fn": round(avg_fn, 1),
        "maintainability":  round(min(score, 100), 1),
    }


# â”€â”€â”€ Report rendering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

W = 72

def _hr(c: str = "â”€") -> str:   return c * W
def _center(t: str) -> str:     return t.center(W)
def _row(label: str, value: Any, width: int = 44) -> str:
    label, value = str(label), str(value)
    return f"  {label}{'.' * max(1, width - len(label))} {value}"
def _bar(pct: float, width: int = 28) -> str:
    pct    = max(0.0, min(100.0, pct))
    filled = round(pct / 100 * width)
    return "â–ˆ" * filled + "â–‘" * (width - filled)


def print_full_report(
    doc_metrics:     list[DocumentMetrics],
    item_stats_list: list[dict],
    coverage:        dict | None,
    timing:          dict,
    code_quality:    dict,
    has_gt:          bool,
) -> dict:
    """Render the full evaluation dashboard and return the report dict."""

    print("\n" + "â•" * W)
    print(_center("  HOSPITAL BILL EXTRACTOR â€” EVALUATION REPORT v4  "))
    print("â•" * W)

    overall_acc = precision = recall = f1 = 0.0
    total_tp = total_tn = total_fp = total_fn = 0
    threshold_met = False
    item_acc = 0.0
    agg: dict = {}

    # â”€â”€ SECTION 1: Accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'â–Œ ACCURACY METRICS':^{W}}")
    print(_hr())

    if has_gt and doc_metrics:
        total_tp = sum(d.tp for d in doc_metrics)
        total_tn = sum(d.tn for d in doc_metrics)
        total_fp = sum(d.fp for d in doc_metrics)
        total_fn = sum(d.fn for d in doc_metrics)
        total    = total_tp + total_tn + total_fp + total_fn

        overall_acc = (total_tp + total_tn) / total if total else 0
        precision   = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0
        recall      = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0
        f1          = 2 * precision * recall / (precision + recall) if (precision + recall) else 0

        print(_row("True  Positives  (TP) â€” Correct extractions",    total_tp))
        print(_row("True  Negatives  (TN) â€” Correct 'not present'",  total_tn))
        print(_row("False Positives  (FP) â€” Wrong value extracted",   total_fp))
        print(_row("False Negatives  (FN) â€” Missed real values",      total_fn))
        print(_hr("Â·"))
        print(_row("Overall Accuracy",       f"{overall_acc*100:.2f}%"))
        print(_row("Precision",              f"{precision*100:.2f}%"))
        print(_row("Recall (Sensitivity)",   f"{recall*100:.2f}%"))
        print(_row("F1 Score",               f"{f1*100:.2f}%"))
        print(f"\n  Accuracy  {_bar(overall_acc*100)}  {overall_acc*100:.1f}%")

        threshold_met = overall_acc >= 0.95
        status = "âœ… PASS â€” â‰¥95% threshold met" if threshold_met else "âŒ FAIL â€” below 95%"
        print(f"\n  Qualification Status: {status}")

        # Per-document breakdown
        print(f"\n  {'â”€ Per-Document Breakdown ':â”€<{W-2}}")
        print(f"  {'File':<32} {'TP':>4} {'TN':>4} {'FP':>4} {'FN':>4} {'Acc':>7} {'F1':>7}")
        print(f"  {'-'*66}")
        for dm in doc_metrics:
            print(
                f"  {dm.filename[:31]:<32}"
                f" {dm.tp:>4} {dm.tn:>4} {dm.fp:>4} {dm.fn:>4}"
                f" {dm.accuracy*100:>6.1f}% {dm.f1*100:>6.1f}%"
            )

        # Per-field breakdown with FP/FN diagnostics (FIX F)
        print(f"\n  {'â”€ Per-Field Accuracy (âš  = <80%, fix extractor) ':â”€<{W-2}}")
        field_stats: dict[str, dict] = {}
        for dm in doc_metrics:
            for fr in dm.field_results:
                s = field_stats.setdefault(
                    fr.field,
                    {"TP": 0, "TN": 0, "FP": 0, "FN": 0, "fp_ex": [], "fn_ex": []}
                )
                s[fr.outcome] += 1
                if fr.outcome == "FP" and len(s["fp_ex"]) < 3:
                    s["fp_ex"].append(
                        f"got={str(fr.extracted)[:30]!r}  exp={str(fr.expected)[:30]!r}"
                    )
                if fr.outcome == "FN" and len(s["fn_ex"]) < 3:
                    s["fn_ex"].append(f"exp={str(fr.expected)[:40]!r}")

        print(f"  {'Field':<46} {'TP':>3} {'TN':>3} {'FP':>3} {'FN':>3} {'Acc':>7}")
        print(f"  {'-'*67}")
        for fname, s in sorted(field_stats.items()):
            t   = sum(v for k, v in s.items() if k in ("TP","TN","FP","FN"))
            acc = (s["TP"] + s["TN"]) / t * 100 if t else 0
            flag = " âš " if acc < 80 else ""
            print(f"  {fname:<46} {s['TP']:>3} {s['TN']:>3} {s['FP']:>3} {s['FN']:>3} {acc:>6.0f}%{flag}")
            if acc < 80:
                for ex in s["fp_ex"]: print(f"      FP: {ex}")
                for ex in s["fn_ex"]: print(f"      FN: {ex}")

        # Line-item accuracy (FIX E)
        if item_stats_list:
            print(f"\n  {'â”€ Line-Item Accuracy (best-of-N fuzzy, Jaccard â‰¥0.35) ':â”€<{W-2}}")
            agg = {}
            for k in ["gt_count","extracted_count","matched_full",
                      "matched_desc_only","matched_amt_only","missed","spurious"]:
                agg[k] = sum(d.get(k, 0) for d in item_stats_list)
            agg["total_credit"] = round(
                sum(d.get("total_credit", 0) for d in item_stats_list), 2
            )
            item_acc = agg["total_credit"] / agg["gt_count"] * 100 if agg["gt_count"] else 0
            print(_row("GT line items (total)",              agg["gt_count"]))
            print(_row("Extracted line items",               agg["extracted_count"]))
            print(_row("Matched full (desc+amt, Ã—1.0)",      agg["matched_full"]))
            print(_row("Matched desc only (Ã—0.7)",           agg["matched_desc_only"]))
            print(_row("Matched amount only (Ã—0.4)",         agg["matched_amt_only"]))
            print(_row("Missed (FN)",                        agg["missed"]))
            print(_row("Spurious (FP)",                      agg["spurious"]))
            print(f"\n  Item Accuracy (credit-weighted)  {_bar(item_acc)}  {item_acc:.1f}%")

    else:
        print("  â„¹  No ground truth â€” showing FIELD COVERAGE.")
        if coverage:
            print(f"\n  {'Field':<46} {'Coverage':>10}  Bar")
            print(f"  {'-'*69}")
            for fname, info in coverage.items():
                pct  = info["coverage_pct"]
                flag = " âš " if pct < 60 else ""
                print(f"  {fname:<46} {pct:>8.1f}%  {_bar(pct, 16)}{flag}")
            avg_cov = statistics.mean(v["coverage_pct"] for v in coverage.values())
            print(f"\n  Average field coverage: {avg_cov:.1f}%")

    # â”€â”€ SECTION 2: Response Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n\n{'â–Œ RESPONSE TIME METRICS':^{W}}")
    print(_hr())
    print(_row("Files processed", timing["total_files"]))
    print(_row("Total wall time", f"{timing['total_s']:.2f}s"))
    print(_row("Average per file", f"{timing['avg_s']:.2f}s"))
    print(_row("Fastest file", f"{timing['min_s']:.2f}s"))
    print(_row("Slowest file", f"{timing['max_s']:.2f}s"))
    if "p50_s" in timing:
        print(_row("Median (P50)", f"{timing['p50_s']:.2f}s"))
        print(_row("P90 response time", f"{timing['p90_s']:.2f}s"))
    avg = timing["avg_s"]
    speed = ("ğŸš€ Excellent (< 5s/doc)"    if avg < 5  else
             "âœ… Good (< 15s/doc)"        if avg < 15 else
             "âš   Acceptable (< 30s/doc)"  if avg < 30 else
             "âŒ Slow (> 30s/doc)")
    print(f"\n  Speed Rating: {speed}")

    # â”€â”€ SECTION 3: Cost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n\n{'â–Œ COST EFFICIENCY':^{W}}")
    print(_hr())
    print(_row("OCR engine",                          "Tesseract (local, free)"))
    print(_row("API calls",                           "0"))
    print(_row("Cost per document",                   "$0.00"))
    print(_row("Total cost",                          "$0.00"))
    print(_row("Estimated GPT-4V equivalent",         f"~${timing['total_files'] * 0.03:.2f}"))
    print(_row("Savings vs cloud OCR",               "100%"))
    print("\n  Cost Rating: ğŸ† Maximum (free local processing)")

    # â”€â”€ SECTION 4: Code Quality â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n\n{'â–Œ CODE QUALITY METRICS':^{W}}")
    print(_hr())
    if "error" not in code_quality:
        lm = code_quality.get("line_metrics", {})
        fm = code_quality.get("function_metrics", {})
        print(_row("Script",                  code_quality.get("file", "N/A")))
        print(_row("Total lines",             lm.get("total_lines", "N/A")))
        print(_row("Code lines",              lm.get("code_lines", "N/A")))
        print(_row("Comment lines",           lm.get("comment_lines", "N/A")))
        print(_row("Comment ratio",           f"{lm.get('comment_ratio',0):.1f}%"))
        print(_hr("Â·"))
        print(_row("Functions defined",       fm.get("functions", "N/A")))
        print(_row("Classes defined",         fm.get("classes", "N/A")))
        print(_row("Functions with docstrings",
                   f"{fm.get('documented_funcs','N/A')} / {fm.get('functions','?')} "
                   f"({fm.get('documentation_pct',0):.0f}%)"))
        print(_row("Avg lines per function",  f"{code_quality.get('avg_lines_per_fn','N/A')}"))
        maint = code_quality.get("maintainability", 0)
        print(f"\n  Maintainability  {_bar(maint)}  {maint:.0f}/100")
        grade = ("A (Excellent)" if maint >= 85 else "B (Good)"    if maint >= 70 else
                 "C (Fair)"     if maint >= 55 else "D (Needs work)")
        print(f"  Grade: {grade}")
    else:
        print(f"  âš   {code_quality['error']}")

    # â”€â”€ SECTION 5: Weighted Scorecard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n\n{'â–Œ SCORECARD SUMMARY (weighted)':^{W}}")
    print(_hr("â•"))

    scores:  dict[str, float] = {}
    weights: dict[str, float] = {}

    if has_gt and doc_metrics:
        scores["Accuracy (field)"]    = overall_acc * 100
        weights["Accuracy (field)"]   = 2.0
        if item_stats_list:
            scores["Line-Item Accuracy"]  = item_acc
            weights["Line-Item Accuracy"] = 1.5
    else:
        avg_cov = statistics.mean(v["coverage_pct"] for v in coverage.values()) if coverage else 0
        scores["Field Coverage (proxy)"]  = avg_cov
        weights["Field Coverage (proxy)"] = 2.0

    scores["Cost Efficiency"]   = 100.0
    weights["Cost Efficiency"]  = 1.0
    scores["Response Time"]     = min(100, max(0, 100 - (timing["avg_s"] / 30) * 50))
    weights["Response Time"]    = 1.0
    scores["Code Quality"]      = code_quality.get("maintainability", 70)
    weights["Code Quality"]     = 1.0

    weighted_sum  = sum(scores[k] * weights[k] for k in scores)
    total_weight  = sum(weights.values())
    overall_score = weighted_sum / total_weight

    for metric, score in scores.items():
        w = weights[metric]
        label = f"{metric} (Ã—{w:.0f})" if w != 1.0 else metric
        print(f"  {label:<36}  {_bar(score, 24)}  {score:.1f}/100")
    print(_hr("Â·"))
    print(f"  {'WEIGHTED OVERALL SCORE':<36}  {_bar(overall_score, 24)}  {overall_score:.1f}/100")
    print("â•" * W + "\n")

    # Build report dict
    report: dict[str, Any] = {
        "generated_at":  __import__("datetime").datetime.now().isoformat(),
        "timing":        timing,
        "cost":          {"total_usd": 0.0, "per_doc": 0.0, "engine": "tesseract_local"},
        "code_quality":  code_quality,
        "scores":        {k: round(v, 2) for k, v in scores.items()},
        "weights":       weights,
        "overall_score": round(overall_score, 2),
    }

    if has_gt and doc_metrics:
        report["accuracy"] = {
            "TP": total_tp, "TN": total_tn, "FP": total_fp, "FN": total_fn,
            "overall_pct":   round(overall_acc * 100, 2),
            "precision_pct": round(precision   * 100, 2),
            "recall_pct":    round(recall      * 100, 2),
            "f1_pct":        round(f1          * 100, 2),
            "threshold_met": threshold_met,
        }
        report["per_document"] = [
            {"file": dm.filename, "time_s": dm.processing_s,
             "TP": dm.tp, "TN": dm.tn, "FP": dm.fp, "FN": dm.fn,
             "accuracy": round(dm.accuracy * 100, 2), "f1": round(dm.f1 * 100, 2)}
            for dm in doc_metrics
        ]
        if item_stats_list:
            report["line_item_accuracy"] = {**agg, "pct": round(item_acc, 2)}
    else:
        report["field_coverage"] = coverage

    return report


def main() -> None:
    """Main entry point: load files, evaluate, print dashboard, save report."""
    print("=" * W)
    print(_center("  EVALUATION ENGINE v4 STARTING  "))
    print("=" * W)

    extracted_files = sorted([
        f for f in OUT_DIR.glob("*.json") if not f.name.startswith("_")
    ])
    if not extracted_files:
        print(f"\n  âœ—  No extracted JSON files found in {OUT_DIR}")
        sys.exit(1)
    print(f"\n  Found {len(extracted_files)} extracted file(s) in {OUT_DIR}")

    times: list[float] = []
    for fp in extracted_files:
        try:
            t = json.loads(fp.read_text(encoding="utf-8")).get("_meta", {}).get("processing_time_s")
            if t is not None:
                times.append(float(t))
        except Exception:
            pass
    if not times:
        times = [0.0]

    timing: dict[str, Any] = {
        "total_files": len(extracted_files),
        "total_s":     round(sum(times), 2),
        "avg_s":       round(statistics.mean(times), 2),
        "min_s":       round(min(times), 2),
        "max_s":       round(max(times), 2),
    }
    if len(times) >= 2:
        st = sorted(times)
        timing["p50_s"] = round(statistics.median(st), 2)
        timing["p90_s"] = round(st[math.ceil(len(st) * 0.9) - 1], 2)

    gt_files = list(GROUND_TRUTH.glob("*.json")) if GROUND_TRUTH.exists() else []
    has_gt   = len(gt_files) > 0

    doc_metrics:     list[DocumentMetrics] = []
    item_stats_list: list[dict]            = []
    coverage:        dict | None           = None

    if has_gt:
        print(f"  Found {len(gt_files)} ground-truth file(s) â€” full accuracy mode.\n")
        for gt_fp in gt_files:
            ext_fp = OUT_DIR / gt_fp.name
            if not ext_fp.exists():
                print(f"  âš   No extracted file for GT: {gt_fp.name} â€” skipping")
                continue
            try:
                extracted = json.loads(ext_fp.read_text(encoding="utf-8"))
                gt        = json.loads(gt_fp.read_text(encoding="utf-8"))
                proc_time = extracted.get("_meta", {}).get("processing_time_s", 0)
                dm, istats = evaluate_document(extracted, gt, gt_fp.name, proc_time)
                doc_metrics.append(dm)
                if istats:
                    item_stats_list.append(istats)
            except Exception as exc:
                import traceback as tb
                print(f"  âœ—  Error evaluating {gt_fp.name}: {exc}")
                print(tb.format_exc())
    else:
        print("  â„¹  No ground truth â€” computing field coverage.\n")
        coverage = coverage_report(extracted_files)

    cq     = analyze_code_quality(EXTRACTOR_SCRIPT)
    report = print_full_report(doc_metrics, item_stats_list, coverage, timing, cq, has_gt)

    report_path = OUT_DIR / "_evaluation_report_v4.json"
    report_path.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"  Full report saved â†’ {report_path}\n")


if __name__ == "__main__":
    main()